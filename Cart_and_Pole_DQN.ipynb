{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Cart and Pole DQN.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOHy3/lpYQfcvRRjOG5cRyX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/suprabhat25/FrozenLake/blob/main/Cart_and_Pole_DQN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MrMdEY0OIMtI"
      },
      "source": [
        "Import Libraries\n",
        "https://deeplizard.com/learn/video/PyQNfsGUnQA\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0NIzCVISFcNN"
      },
      "source": [
        "!apt-get install -y xvfb python-opengl > /dev/null 2>&1"
      ],
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iW-NCm7FFjG5"
      },
      "source": [
        "!pip install gym pyvirtualdisplay > /dev/null 2>&1"
      ],
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cZehEeTwIX6x",
        "outputId": "bb07bafa-029e-4928-b5a0-254679c2168c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        }
      },
      "source": [
        "!apt-get install python-opengl -y\n",
        "\n",
        "!apt install xvfb -y\n",
        "\n",
        "!pip install pyvirtualdisplay\n",
        "\n",
        "!pip install piglet"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "python-opengl is already the newest version (3.1.0+dfsg-1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 21 not upgraded.\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "xvfb is already the newest version (2:1.19.6-1ubuntu4.7).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 21 not upgraded.\n",
            "Requirement already satisfied: pyvirtualdisplay in /usr/local/lib/python3.6/dist-packages (1.3.2)\n",
            "Requirement already satisfied: EasyProcess in /usr/local/lib/python3.6/dist-packages (from pyvirtualdisplay) (0.3)\n",
            "Requirement already satisfied: piglet in /usr/local/lib/python3.6/dist-packages (1.0.0)\n",
            "Requirement already satisfied: piglet-templates in /usr/local/lib/python3.6/dist-packages (from piglet) (1.0.0)\n",
            "Requirement already satisfied: markupsafe in /usr/local/lib/python3.6/dist-packages (from piglet-templates->piglet) (1.1.1)\n",
            "Requirement already satisfied: attrs in /usr/local/lib/python3.6/dist-packages (from piglet-templates->piglet) (20.2.0)\n",
            "Requirement already satisfied: Parsley in /usr/local/lib/python3.6/dist-packages (from piglet-templates->piglet) (1.3)\n",
            "Requirement already satisfied: astunparse in /usr/local/lib/python3.6/dist-packages (from piglet-templates->piglet) (1.6.3)\n",
            "Requirement already satisfied: six<2.0,>=1.6.1 in /usr/local/lib/python3.6/dist-packages (from astunparse->piglet-templates->piglet) (1.15.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.6/dist-packages (from astunparse->piglet-templates->piglet) (0.35.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2mogfMwyIPG4"
      },
      "source": [
        "%matplotlib inline\n",
        "import gym\n",
        "import math \n",
        "import random \n",
        "import numpy as np\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import namedtuple\n",
        "from itertools import count\n",
        "from PIL import Image\n",
        "import torch\n",
        "import torch.nn as nn # To build a neural network in PyTorch, we use the torch.nn package  \n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms as T"
      ],
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VapKMiieJDMD"
      },
      "source": [
        "Set up display"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CjWXphcGI_fn"
      },
      "source": [
        "is_ipython = 'inline' in matplotlib.get_backend()\n",
        "if is_ipython: from IPython import display # we import IPython’s display module to aid us in plotting images to the screen later."
      ],
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h1cmih-DJW6z"
      },
      "source": [
        "Deep Q-Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kRpGABkoJWAo"
      },
      "source": [
        "#Module is the base class for all NN\n",
        "#DQN receives screenshots of current environment as inputs\n",
        "#so, img's height and width has been used as arguments.\n",
        "class DQN(nn.Module):\n",
        "  def __init__(self, img_height, img_width):\n",
        "    super().__init__()\n",
        "\n",
        "    self.fc1 = nn.Linear(in_features=img_height*img_width*3, out_features=24)\n",
        "    self.fc2 = nn.Linear(in_features=24, out_features=32)\n",
        "    self.out = nn.Linear(in_features=32, out_features=2)\n",
        "\n",
        "  def forward(self, t):\n",
        "    t = t.flatten(start_dim=1)  # https://stackoverflow.com/questions/43237124/what-is-the-role-of-flatten-in-keras\n",
        "    # Flattening a tensor means to remove all of the dimensions except for one. This is exactly what the Flatten layer do.\n",
        "    t = F.relu(self.fc1(t))\n",
        "    t = F.relu(self.fc2(t))\n",
        "    t = self.out(t)\n",
        "    return t\n",
        "\n",
        "    # In our particular cart and pole example, remember that the network will be outputting the Q-values \n",
        "    # that correspond to each possible action that the agent can take from a given state. Our only available actions are to move right \n",
        "    # or to move left, therefore, the number outputs will be equal to two.\n",
        "\n",
        "\n",
        "  # To start out with a very simple network, our network will consist only of two fully connected hidden layers, \n",
        "  # and an output layer. PyTorch refers to fully connected layers as Linear layers.\n",
        "\n",
        "  # The last thing we have to do for our DQN class is to define a function called forward(). This function will implement a forward pass to the network.\n",
        "  # Note that all PyTorch neural networks require an implementation of forward()."
      ],
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SAXg0f5nMH7R"
      },
      "source": [
        "Experience class from replay memory will be used to train the NN."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DuHjmmr0MFsC"
      },
      "source": [
        "Experience = namedtuple (# experiences from replay memory is what we’ll use to train our network.\n",
        " \n",
        "    'Experience', ('state', 'action', 'next_state', 'reward')\n",
        "  )"
      ],
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zi1b9wb_M7U_"
      },
      "source": [
        "e = Experience(2,3,1,4)"
      ],
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8G_7rC-7NDVy",
        "outputId": "cd0d988d-f20b-45f5-cc47-4eef49aa2bd2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "e"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Experience(state=2, action=3, next_state=1, reward=4)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X5FEPrkBNOG_"
      },
      "source": [
        "class ReplayMemory(): # ReplayMemory class, which is where these experiences will be stored.\n",
        "  def __init__(self,capacity):\n",
        "    self.capacity = capacity\n",
        "    self.memory = [] # stores the experiences of the agent \n",
        "    self.push_count = 0 #keeps track of how many experiences we have added so far to the memory\n",
        "  \n",
        "  def push(self, experience):\n",
        "    if len(self.memory) < self.capacity:\n",
        "      self.memory.append(experience)\n",
        "    else:\n",
        "      self.memory[self.push_count % self.capacity] = experience # used to override the older experiences. It means experiences will be added at the front of the memory.\n",
        "    self.push_count += 1\n",
        "\n",
        "  def sample(self,batch_size): #sample experiences are used to train our DQN\n",
        "    return random.sample(self.memory, batch_size)\n",
        "\n",
        "  def can_provide_sample(self, batch_size):\n",
        "    return len(self.memory) >= batch_size\n"
      ],
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IPeDNVCWQwff"
      },
      "source": [
        "Epsilon Greedy Strategy\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hfnbz1EMQtgz"
      },
      "source": [
        "class EpsilonGreedyStrategy():\n",
        "  def __init__(self, start, end, decay):\n",
        "    self.start = start\n",
        "    self.end = end\n",
        "    self.decay = decay\n",
        "\n",
        "  def get_exploration_rate(self, current_step): #this decay rate explains what to prefer \"exploration\" or \"exploitation\"\n",
        "    return self.end + (self.start - self.end) * math.exp(-1 * current_step * self.decay)"
      ],
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HKl2ToakTlAP"
      },
      "source": [
        "Reinforcement Learning Agent"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sPAXpa_uTjuW"
      },
      "source": [
        "class Agent():\n",
        "  def __init__(self, strategy, num_actions, device ): #strategy used here is taking values from EpsilonGreedyStrategy()\n",
        "  # num_actions corresponds to how many actions an agent can take from a given state.\n",
        "  # In the cart pole game, we'll always have 2 num_actions as agent can only go in either left or right directions.\n",
        "    self.current_step = 0 \n",
        "    self.strategy = strategy\n",
        "    self.num_actions = num_actions\n",
        "    self.device = device\n",
        "\n",
        "  def select_action(self, state, policy_net): # policy_net is the policy network we used to train the DQ network to learn optimal policy.\n",
        "    rate = strategy.get_exploration_rate(self.current_step) # current_step as an argument tells which option to go for \"exploration\" or \"exploitation\".\n",
        "    self.current_step += 1\n",
        "\n",
        "    if rate > random.random():\n",
        "      return random.randrange(self.num_actions) # we explore the environment by randomly selecting an action\n",
        "    else:\n",
        "      with torch.no_grad(): #using this to turn off gradient tracking since we r using the model just\n",
        "      # for inference and not for training.\n",
        "        return policy_net(state).argmax(dim=1).to(device) #exploit\n",
        "        # we exploit the environment by selecting the action that corresponds to highest Q-value \n",
        "        # output from our policy network for the given state.\n",
        "\n",
        "\n",
        "\n",
        "  # During training PyTorch keeps track of all the forward pass calculations that happen within the network. It needs to do this so that it can know how to apply backpropagation later. \n",
        "  # Since we’re only using the model for inference at the moment, we’re telling PyTorch not to keep track of any forward pass calculations.\n",
        "\n",
        "  # INFERENCE is the process of drawing conclusions about a parameter one is seeking to measure or estimate."
      ],
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "om4ZR-LBsSta"
      },
      "source": [
        "Environment Manager"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vKJrsOEYrn7n"
      },
      "source": [
        " # this class will manage our cart and pole environment\n",
        "class CartPoleEnvManager():\n",
        "   def __init__(self, device):\n",
        "        self.device = device\n",
        "        self.env = gym.make('CartPole-v0').unwrapped\n",
        "        self.env.reset()\n",
        "        self.current_screen = None\n",
        "        self.done = False\n",
        "   def reset(self):\n",
        "    self.env.reset()\n",
        "    self.current_screen = None\n",
        "\n",
        "   def close(self):\n",
        "    self.env.close()\n",
        "\n",
        "   def render(self, mode='human'):\n",
        "    return self.env.render(mode)\n",
        "\n",
        "   def num_actions_available(self):\n",
        "    return self.env.action_space.n\n",
        "\n",
        "   def take_action(self, action):        \n",
        "    _, reward, self.done, _ = self.env.step(action.item())\n",
        "    return torch.tensor([reward], device=self.device)\n",
        "\n",
        "   def just_starting(self):\n",
        "    return self.current_screen is None\n",
        "\n",
        "   def get_state(self):\n",
        "    if self.just_starting() or self.done:\n",
        "        self.current_screen = self.get_processed_screen()\n",
        "        black_screen = torch.zeros_like(self.current_screen)\n",
        "        return black_screen\n",
        "    else:\n",
        "        s1 = self.current_screen\n",
        "        s2 = self.get_processed_screen()\n",
        "        self.current_screen = s2\n",
        "        return s2 - s1\n",
        "\n",
        "    def get_screen_height(self):\n",
        "      screen = self.get_processed_screen()\n",
        "      return screen.shape[2]\n",
        "\n",
        "    def get_screen_width(self):\n",
        "      screen = self.get_processed_screen()\n",
        "      return screen.shape[3]\n",
        "\n",
        "    def get_processed_screen(self):\n",
        "     screen = self.render('rgb_array').transpose((2, 0, 1)) # PyTorch expects CHW\n",
        "     screen = self.crop_screen(screen)\n",
        "     return self.transform_screen_data(screen)\n",
        "\n",
        "\n",
        "    def crop_screen(self, screen):\n",
        "     screen_height = screen.shape[1]\n",
        "\n",
        "    # Strip off top and bottom\n",
        "     top = int(screen_height * 0.4)\n",
        "     bottom = int(screen_height * 0.8)\n",
        "     screen = screen[:, top:bottom, :]\n",
        "     return screen\n",
        "\n",
        "   def transform_screen_data(self, screen):       \n",
        "    # Convert to float, rescale, convert to tensor\n",
        "    screen = np.ascontiguousarray(screen, dtype=np.float32) / 255\n",
        "    screen = torch.from_numpy(screen)\n",
        "\n",
        "    # Use torchvision package to compose image transforms\n",
        "    resize = T.Compose([\n",
        "        T.ToPILImage()\n",
        "        ,T.Resize((40,90))\n",
        "        ,T.ToTensor()\n",
        "    ])\n",
        "\n",
        "    return resize(screen).unsqueeze(0).to(self.device) # add a batch dimension (BCHW)\n",
        "\n"
      ],
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V99f14jMENYE"
      },
      "source": [
        "Non-processed Screen"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9l5ohOxEEHOw",
        "outputId": "27235c38-d4b6-4b14-abf5-196ad6d2b460",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 370
        }
      },
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "em = CartPoleEnvManager(device)\n",
        "em.reset()\n",
        "screen = em.render(mode='rgb_array')\n",
        "\n",
        "plt.figure()\n",
        "plt.imshow(screen)\n",
        "plt.title('Non-processed screen example')\n",
        "plt.show()"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-73-7c3d4ca3daac>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCartPoleEnvManager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mscreen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'rgb_array'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-72-21df2a563aed>\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self, mode)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'human'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m    \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mnum_actions_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/gym/envs/classic_control/cartpole.py\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self, mode)\u001b[0m\n\u001b[1;32m    172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mviewer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 174\u001b[0;31m             \u001b[0;32mfrom\u001b[0m \u001b[0mgym\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menvs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassic_control\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrendering\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    175\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mviewer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrendering\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mViewer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscreen_width\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscreen_height\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m             \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mcartwidth\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcartwidth\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcartheight\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mcartheight\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/gym/envs/classic_control/rendering.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0mpyglet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgl\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     raise ImportError('''\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pyglet/gl/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    233\u001b[0m \u001b[0;32melif\u001b[0m \u001b[0mcompat_platform\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'darwin'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mcocoa\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCocoaConfig\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mConfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 235\u001b[0;31m \u001b[0;32mdel\u001b[0m \u001b[0mbase\u001b[0m  \u001b[0;31m# noqa: F821\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    236\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'base' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zfM3EiRPJ8R7"
      },
      "source": [
        "Processed Screen"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nHMfOeoYJ7Eu"
      },
      "source": [
        "screen = em.get_processed_screen()\n",
        "\n",
        "plt.figure()\n",
        "plt.imshow(screen.squeeze(0).permute(1, 2, 0).cpu(), interpolation='none')\n",
        "plt.title('Processed screen example')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5TNMPnc1KFlO"
      },
      "source": [
        "Starting state"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iU9sLE0VKDpw"
      },
      "source": [
        "screen = em.get_state()\n",
        "\n",
        "plt.figure()\n",
        "plt.imshow(screen.squeeze(0).permute(1, 2, 0).cpu(), interpolation='none')\n",
        "plt.title('Starting state example')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mYSsuMKNKOz2"
      },
      "source": [
        "Non-Starting State"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t8EeVOw6KL3E"
      },
      "source": [
        "for i in range(5):\n",
        "    em.take_action(torch.tensor([1]))\n",
        "screen = em.get_state()\n",
        "\n",
        "plt.figure()\n",
        "plt.imshow(screen.squeeze(0).permute(1, 2, 0).cpu(), interpolation='none')\n",
        "plt.title('Non starting state example')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LvoWoPJeKUjs"
      },
      "source": [
        "Ending State"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KAiXJOZFKTOO"
      },
      "source": [
        "em.done = True\n",
        "screen = em.get_state()\n",
        "\n",
        "plt.figure()\n",
        "plt.imshow(screen.squeeze(0).permute(1, 2, 0).cpu(), interpolation='none')\n",
        "plt.title('Ending state example')\n",
        "plt.show()\n",
        "em.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IwJoAFJcKcsq"
      },
      "source": [
        "def plot(values, moving_avg_period):\n",
        "    plt.figure(2)\n",
        "    plt.clf()        \n",
        "    plt.title('Training...')\n",
        "    plt.xlabel('Episode')\n",
        "    plt.ylabel('Duration')\n",
        "    plt.plot(values)\n",
        "    plt.plot(get_moving_average(moving_avg_period, values))\n",
        "    plt.pause(0.001)\n",
        "    if is_ipython: display.clear_output(wait=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ggQNYtqKnFg"
      },
      "source": [
        "def get_moving_average(period, values):\n",
        "    values = torch.tensor(values, dtype=torch.float)\n",
        "    if len(values) >= period:\n",
        "        moving_avg = values.unfold(dimension=0, size=period, step=1) \\\n",
        "            .mean(dim=1).flatten(start_dim=0)\n",
        "        moving_avg = torch.cat((torch.zeros(period-1), moving_avg))\n",
        "        return moving_avg.numpy()\n",
        "    else:\n",
        "        moving_avg = torch.zeros(len(values))\n",
        "        return moving_avg.numpy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "evZLu3USKs-w"
      },
      "source": [
        "plot(np.random.rand(300), 100)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}