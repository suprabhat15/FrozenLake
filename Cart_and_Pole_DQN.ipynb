{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Cart and Pole DQN.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOyeTIRtZDfS7s5VJcMIjIZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/suprabhat25/FrozenLake/blob/main/Cart_and_Pole_DQN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MrMdEY0OIMtI"
      },
      "source": [
        "Import Libraries\n",
        "https://deeplizard.com/learn/video/PyQNfsGUnQA\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2mogfMwyIPG4"
      },
      "source": [
        "%matplotlib inline\n",
        "import gym\n",
        "import math \n",
        "import random \n",
        "import numpy as np\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import namedtuple\n",
        "from itertools import count\n",
        "from PIL import Image\n",
        "import torch\n",
        "import torch.nn as nn # To build a neural network in PyTorch, we use the torch.nn package  \n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms as T"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VapKMiieJDMD"
      },
      "source": [
        "Set up displaly"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CjWXphcGI_fn"
      },
      "source": [
        "is_ipython = 'inline' in matplotlib.get_backend()\n",
        "if is_ipython: from IPython import display # we import IPython’s display module to aid us in plotting images to the screen later."
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h1cmih-DJW6z"
      },
      "source": [
        "Deep Q-Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kRpGABkoJWAo"
      },
      "source": [
        "#Module is the base class for all NN\n",
        "#DQN receives screenshots of current environment as inputs\n",
        "#so, img's height and width has been used as arguments.\n",
        "class DQN(nn.Module):\n",
        "  def __init__(self, img_height, img_width):\n",
        "    super().__init__()\n",
        "\n",
        "    self.fc1 = nn.Linear(in_features=img_height*img_width*3, out_features=24)\n",
        "    self.fc2 = nn.Linear(in_features=24, out_features=32)\n",
        "    self.out = nn.Linear(in_features=32, out_features=2)\n",
        "\n",
        "  def forward(self, t):\n",
        "    t = t.flatten(start_dim=1)  # https://stackoverflow.com/questions/43237124/what-is-the-role-of-flatten-in-keras\n",
        "    # Flattening a tensor means to remove all of the dimensions except for one. This is exactly what the Flatten layer do.\n",
        "    t = F.relu(self.fc1(t))\n",
        "    t = F.relu(self.fc2(t))\n",
        "    t = self.out(t)\n",
        "    return t\n",
        "\n",
        "    # In our particular cart and pole example, remember that the network will be outputting the Q-values \n",
        "    # that correspond to each possible action that the agent can take from a given state. Our only available actions are to move right \n",
        "    # or to move left, therefore, the number outputs will be equal to two.\n",
        "\n",
        "\n",
        "  # To start out with a very simple network, our network will consist only of two fully connected hidden layers, \n",
        "  # and an output layer. PyTorch refers to fully connected layers as Linear layers.\n",
        "\n",
        "  # The last thing we have to do for our DQN class is to define a function called forward(). This function will implement a forward pass to the network.\n",
        "  # Note that all PyTorch neural networks require an implementation of forward()."
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SAXg0f5nMH7R"
      },
      "source": [
        "Experience class from replay memory will be used to train the NN."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DuHjmmr0MFsC"
      },
      "source": [
        "Experience = namedtuple # experiences from replay memory is what we’ll use to train our network.\n",
        " (\n",
        "    'Experience', ('state', 'action', 'next_state', 'reward')\n",
        "  )"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zi1b9wb_M7U_"
      },
      "source": [
        "e = Experience(2,3,1,4)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8G_7rC-7NDVy",
        "outputId": "37801e12-9d8f-4678-a531-7d93e1061daa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "e"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Experience(state=2, action=3, next_state=1, reward=4)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X5FEPrkBNOG_"
      },
      "source": [
        "class ReplayMemory(): # ReplayMemory class, which is where these experiences will be stored.\n",
        "  def __init__(self,capacity):\n",
        "    self.capacity = capacity\n",
        "    self.memory = [] # stores the experiences of the agent \n",
        "    self.push_count = 0 #keeps track of how many experiences we have added so far to the memory\n",
        "  \n",
        "  def push(self, experience):\n",
        "    if len(self.memory) < self.capacity:\n",
        "      self.memory.append(experience)\n",
        "    else:\n",
        "      self.memory[self.push_count % self.capacity] = experience # used to override the older experiences. It means experiences will be added at the front of the memory.\n",
        "    self.push_count += 1\n",
        "\n",
        "  def sample(self,batch_size): #sample experiences are used to train our DQN\n",
        "    return random.sample(self.memory, batch_size)\n",
        "\n",
        "  def can_provide_sample(self, batch_size):\n",
        "    return len(self.memory) >= batch_size\n"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IPeDNVCWQwff"
      },
      "source": [
        "Epsilon Greedy Strategy\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hfnbz1EMQtgz"
      },
      "source": [
        "class EpsilonGreedyStrategy():\n",
        "  def __init__(self, start, end, decay):\n",
        "    self.start = start\n",
        "    self.end = end\n",
        "    self.decay = decay\n",
        "\n",
        "  def get_exploration_rate(self, current_step): #this decay rate explains what to prefer \"exploration\" or \"exploitation\"\n",
        "    return self.end + (self.start - self.end) * math.exp(-1 * current_step * self.decay)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HKl2ToakTlAP"
      },
      "source": [
        "Reinforcement Learning Agent"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sPAXpa_uTjuW"
      },
      "source": [
        "class Agent():\n",
        "  def __init__(self, strategy, num_actions): #strategy used here is taking values from EpsilonGreedyStrategy()\n",
        "  # num_actions corresponds to how many actions an agent can take from a given state.\n",
        "  # In the cart pole game, we'll always have 2 num_actions as agent can only go in either left or right directions.\n",
        "    self.current_step = 0 \n",
        "    self.strategy = strategy\n",
        "    self.num_actions = num_actions\n",
        "\n",
        "  def select_action(self, state, policy_net): # policy_net is the policy network we used to train the DQ network to learn optimal policy.\n",
        "    rate = strategy.get_exploration_rate(self.current_step) # current_step as an argument tells which option to go for \"exploration\" or \"exploitation\".\n",
        "    self.current_step += 1\n",
        "\n",
        "    if rate > random.random():\n",
        "      return random.randrange(self.num_actions) # we explore the environment by randomly selecting an action\n",
        "    else:\n",
        "      with torch.no_grad(): #using this to turn off gradient tracking since we r using the model just\n",
        "      # for inference and not for training.\n",
        "        return policy_net(state).argmax(dim=1).item() #exploit\n",
        "        # we exploit the environment by selecting the action that corresponds to highest Q-value \n",
        "        # output from our policy network for the given state.\n",
        "\n",
        "\n",
        "\n",
        "  # During training PyTorch keeps track of all the forward pass calculations that happen within the network. It needs to do this so that it can know how to apply backpropagation later. \n",
        "  # Since we’re only using the model for inference at the moment, we’re telling PyTorch not to keep track of any forward pass calculations.\n",
        "\n",
        "  # INFERENCE is the process of drawing conclusions about a parameter one is seeking to measure or estimate."
      ],
      "execution_count": 20,
      "outputs": []
    }
  ]
}